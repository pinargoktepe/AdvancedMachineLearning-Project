{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATML Project Report\n",
    "#### Dusan Mihajlov, Pinar Goktepe, Michael Baur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloth Classification with Self Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Goal: Our goal is to employ self supervised learning methodology to provide better classification performance with less number of labeled data. For both pretext task and target task, the dataset named Deep Fashion (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) is used. This dataset contains 289,222 images with 50 categories. As pretext task, 'a x a' jigsaw puzzle is solved and target task is classification of 50 types of clothes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepFashion dataset is divided into two subdatasets with a ratio of 70%-30%. The larger dataset is used while solving the pretext task and target task which is classification is done on smaller dataset. The reason for such difference is that pretext task is lack of labels. In order to use this dataset for pretask, they are divided into tiles of size 'a x a' and they are shuffled. As pretext task, the network is asked to order tiles of given image correctly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of the self supervised learning is to solving tasks that do not require labeled images as input for feature learning. After solving those tasks which is called pretext tasks, the extracted features are used by the model that aims to slove target task on labeled data. In our project pretext task is Jigsaw puzzle solving and target task is classification. Since the network weights are transferred between two models, they should share same architecture. Since Alexnet is frequently used for self supervised learning tasks as in [], [], [], we selected this architecture to use for both pretext and target tasks. The reason of alexnet's wide usage for self supervised learning is beacuse it is sufficiently deep for pretext task and not too deep for classification task. This two criteria are important for the performance of whole system. If it is not deep enough, then pretext task will not be able to extract good features without labeled data and if very deep architectures may not be suitable for classification tasks beacuse of computational limits. \n",
    "\n",
    "Dataset splitting is completed in dataset.py, data preprocessing for pretext task is in Datasets.py, training pipeline for pretext task is in self_train.py and training pipeline for target task is in class_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
